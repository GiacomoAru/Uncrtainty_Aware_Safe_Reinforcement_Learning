# Experiment and logging settings
exp-name: "wp_5"     # Nome aggiornato per riflettere il nuovo setting
env-id: "std_small"                           # Environment ID
torch-deterministic: true              # Enforce deterministic behavior in PyTorch
cuda: true                             # Enable CUDA if available

# logs
track: false                           # Enable tracking with Weights & Biases (wandb)
loss-log-interval: 150                 # Save losses metrics every "step_save_losses" step
metrics-log-intervall: 300            
metrics-smoothing: 0.985

# Network architecture settings
q-network-layers: [128, 128, 128]      # Hidden layers for the Q network
actor-network-layers: [128, 128, 128]  # Hidden layers for the actor network

# Algorithm-specific hyperparameters
total-timesteps: 80000                 # Total training timesteps
update-per-step: 1                     # Ridotto per evitare overfitting con pi√π update frequenti

buffer-size: 150000                    # Replay buffer size (puoi valutare 150000 se serve)
gamma: 0.9975                           # Discount factor
tau: 0.005                             # Target network smoothing coefficient
batch-size: 384                        # Ridotto per adattarsi alla maggiore frequenza di training
learning-starts: 3000                  # Ritardato per dare tempo agli agenti di riempire il buffer

policy-lr: 0.001                      # Learning rate for the policy network
q-lr: 0.0005                          # Learning rate for the Q network
policy-frequency: 3                  # Frequency of policy updates (delayed)
target-network-frequency: 1           # Frequency of target network updates
noise-clip: 0.5                        # Noise clipping for target policy smoothing

alpha: 0.2                             # Entropy regularization coefficient (sovrascritto se autotune)
autotune: True                         # Enable automatic tuning of the entropy coefficient
