{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2428efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.buffers import DictReplayBuffer\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "\n",
    "from utils_policy_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423c94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = './config/standard.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd055c9a",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actor_network_layers': [128, 128, 128],\n",
      " 'alpha': 1.0,\n",
      " 'alpha_lr': 0.0004,\n",
      " 'autotune': True,\n",
      " 'batch_size': 256,\n",
      " 'bootstrap_batch_proportion': 0.8,\n",
      " 'buffer_size': 120000,\n",
      " 'cuda': True,\n",
      " 'env_id': 'std',\n",
      " 'exp_name': 'base+wp',\n",
      " 'gamma': 0.995,\n",
      " 'learning_starts': 1000,\n",
      " 'loss_log_interval': 100,\n",
      " 'metrics_log_interval': 300,\n",
      " 'metrics_smoothing': 0.985,\n",
      " 'noise_clip': 0.5,\n",
      " 'policy_frequency': 4,\n",
      " 'policy_lr': 0.0004,\n",
      " 'q_ensemble_n': 5,\n",
      " 'q_lr': 0.0004,\n",
      " 'q_network_layers': [128, 128, 128],\n",
      " 'seed': 61045,\n",
      " 'target_network_frequency': 1,\n",
      " 'tau': 0.005,\n",
      " 'torch_deterministic': True,\n",
      " 'total_timesteps': 50000,\n",
      " 'update_per_step': 1}\n"
     ]
    }
   ],
   "source": [
    "args = parse_args_from_file(CONFIG)\n",
    "args.seed = random.randint(0, 2**16)\n",
    "# args.name = generate_funny_name()\n",
    "\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1cd5e",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcecf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c69761",
   "metadata": {},
   "source": [
    "# Start Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the channel\n",
    "env_info = CustomChannel()\n",
    "\n",
    "# env setup\n",
    "env = UnityEnvironment(None, seed=args.seed, side_channels=[env_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398130bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b073c",
   "metadata": {},
   "source": [
    "# Environment Variables and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_name = f\"{args.exp_name}_{int(time.time()) - 1751796000}\"\n",
    "args.full_name = run_name\n",
    "\n",
    "# writer to track performance\n",
    "writer = SummaryWriter(f\"ens_train/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"Algorithm Hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "for dict in env_info.settings:\n",
    "    writer.add_text(\n",
    "        dict,\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in env_info.settings[dict].items()])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the correct agent's behavoir (works if there is only one behavoir)\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "if len(behavior_names) > 1:\n",
    "    raise Exception(\"Multiple behaviors found.\")\n",
    "BEHAVIOUR_NAME = behavior_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEHAVIOUR_NAME = env_info.settings['behavoir_parameters_settings']['behavior_name']\n",
    "\n",
    "RAY_STACK = env_info.settings['ray_sensor_settings']['observation_stacks']\n",
    "RAY_PER_DIRECTION = env_info.settings['ray_sensor_settings']['rays_per_direction']\n",
    "RAYCAST_MIN = env_info.settings['ray_sensor_settings']['min_observation']\n",
    "RAYCAST_MAX = env_info.settings['ray_sensor_settings']['max_observation']\n",
    "DELETE_LAST_RAY = env_info.settings['ray_sensor_settings']['ignore_last_ray']\n",
    "\n",
    "\n",
    "STATE_STACK = env_info.settings['behavoir_parameters_settings']['stacked_vector']\n",
    "STATE_SIZE = env_info.settings['behavoir_parameters_settings']['observation_size']\n",
    "STATE_MIN = env_info.settings['behavoir_parameters_settings']['min_observation']\n",
    "STATE_MAX = env_info.settings['behavoir_parameters_settings']['max_observation']\n",
    "\n",
    "ACTION_SIZE = env_info.settings['behavoir_parameters_settings']['continuous_actions']\n",
    "ACTION_MIN = env_info.settings['behavoir_parameters_settings']['min_action']\n",
    "ACTION_MAX = env_info.settings['behavoir_parameters_settings']['max_action']\n",
    "\n",
    "if DELETE_LAST_RAY:\n",
    "    RAYCAST_SHAPE = (RAY_STACK, 2*RAY_PER_DIRECTION) \n",
    "else:\n",
    "    RAYCAST_SHAPE = (RAY_STACK, 2*RAY_PER_DIRECTION + 1) \n",
    "STATE_SHAPE = (STATE_SIZE*STATE_STACK, )\n",
    "ACTION_SHAPE = (ACTION_SIZE, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating the training networks\n",
    "actor = DenseActor(RAYCAST_SHAPE, STATE_SHAPE[0], ACTION_SHAPE[0], ACTION_MIN, ACTION_MAX, args.actor_network_layers).to(device)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
    "\n",
    "qf_ensemble = [DenseSoftQNetwork(RAYCAST_SHAPE, STATE_SHAPE[0], ACTION_SHAPE[0], args.q_network_layers).to(device) for _ in range(args.q_ensemble_n)]\n",
    "qf_ensemble_target = [DenseSoftQNetwork(RAYCAST_SHAPE, STATE_SHAPE[0], ACTION_SHAPE[0], args.q_network_layers).to(device) for _ in range(args.q_ensemble_n)]\n",
    "for q_t, q in zip(qf_ensemble_target, qf_ensemble):\n",
    "    q_t.load_state_dict(q.state_dict())\n",
    "\n",
    "par = []\n",
    "for q in qf_ensemble:\n",
    "    par += list(q.parameters())\n",
    "qf_optimizer = torch.optim.Adam(\n",
    "    par,\n",
    "    lr=args.q_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754acdf",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb74125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# definition of the gym spaces for the replay buffer\n",
    "observation_space = spaces.Dict({\n",
    "    \"raycast\": spaces.Box(low=RAYCAST_MIN, high=RAYCAST_MAX, shape=RAYCAST_SHAPE, dtype=np.float32),\n",
    "    \"state\": spaces.Box(low=STATE_MIN, high=STATE_MAX, shape=STATE_SHAPE, dtype=np.float32),\n",
    "})\n",
    "action_space = spaces.Box(low=ACTION_MIN, high=ACTION_MAX, shape=ACTION_SHAPE, dtype=np.float32)\n",
    "\n",
    "# initialization of the tailored replay buffer\n",
    "rb = DictReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    device=device,\n",
    "    handle_timeout_termination=True,\n",
    "    optimize_memory_usage=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a11d9",
   "metadata": {},
   "source": [
    "# start algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f83fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = -torch.prod(torch.Tensor(ACTION_SHAPE).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp().clamp(min=1e-4).item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "else:\n",
    "    alpha = args.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bde39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# savepath and variables to the model checkpointing\n",
    "save_path = './new_models/' + run_name\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "best_reward = -float('inf')\n",
    "\n",
    "# to keep track of the episode length\n",
    "episodic_stats = None\n",
    "initial_movements = {}  # agent_id -> movement\n",
    "obs = collect_data_after_step(env, BEHAVIOUR_NAME, RAY_STACK, RAY_PER_DIRECTION, DELETE_LAST_RAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d39f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "print('Start Training')\n",
    "start_time = time.time()\n",
    "global_step = 0\n",
    "while global_step < args.total_timesteps:\n",
    "\n",
    "    # actions for each agent in the environment\n",
    "    # dim = (naagents, action_space)\n",
    "    for id in obs:\n",
    "        agent_obs = obs[id]\n",
    "        \n",
    "        # terminated agents are not considered\n",
    "        if agent_obs[4]:\n",
    "            continue\n",
    "        \n",
    "        # algo logic\n",
    "        if global_step < args.learning_starts * 2:\n",
    "            # change this to use the handcrafted starting policy or a previously trained policy\n",
    "            \n",
    "            action = get_initial_action(id, initial_movements)\n",
    "            # action, _, _ = old_actor.get_action(torch.Tensor([obs[id][0]]), \n",
    "            #                                 torch.Tensor([obs[id][1]]),\n",
    "            #                                 0.5)\n",
    "            # action = action[0].detach().numpy()\n",
    "        else:\n",
    "            # training policy\n",
    "            action, _, _, _ = actor.get_action(torch.Tensor([obs[id][0]]).to(device), \n",
    "                                            torch.Tensor([obs[id][1]]).to(device))\n",
    "            action = action[0].detach().cpu().numpy()\n",
    "        \n",
    "        # memorize the action taken for the next step\n",
    "        agent_obs[3] = action\n",
    "        \n",
    "        # the first dimention of the action is the \"number of agent\"\n",
    "        # Always 1 if \"set_action_for_agent\" is used\n",
    "        a = ActionTuple(continuous=np.array([action]))\n",
    "        env.set_action_for_agent(BEHAVIOUR_NAME, id, a)\n",
    "    \n",
    "    # environment step\n",
    "    env.step()\n",
    "    next_obs = collect_data_after_step(env, BEHAVIOUR_NAME, RAY_STACK, RAY_PER_DIRECTION, DELETE_LAST_RAY)\n",
    "         \n",
    "    # asynchronous reception of other info from ended episode\n",
    "    while env_info.msg_queue:\n",
    "        msg = env_info.msg_queue.pop()\n",
    "        \n",
    "        if global_step >= args.learning_starts:\n",
    "            if episodic_stats == None:\n",
    "                episodic_stats = {\n",
    "                    \"length\": msg[\"length\"],\n",
    "                    \"reward\": msg[\"reward\"],\n",
    "                    \"success\": msg[\"success\"],\n",
    "                    \"collisions\": msg[\"collisions\"],\n",
    "                }\n",
    "            else:\n",
    "                for s in episodic_stats:\n",
    "                    episodic_stats[s] = episodic_stats[s]*args.metrics_smoothing + (1 - args.metrics_smoothing)*msg[s]\n",
    "        \n",
    "    # save data to reply buffer; handle `terminal_observation`\n",
    "    for id in obs:\n",
    "        prev_agent_obs = obs[id]\n",
    "        # consider every agent that in the previous step was not terminated\n",
    "        # in this way are excluded the agents that are already considered before and don't have a \n",
    "        # couple prev_obs - next_obs and a reward\n",
    "        if prev_agent_obs[4] or id not in next_obs:\n",
    "            continue\n",
    "            \n",
    "        next_agent_obs = next_obs[id]\n",
    "        \n",
    "        # add the data to the replay buffer\n",
    "        rb.add(obs = {'raycast': prev_agent_obs[0], 'state': prev_agent_obs[1]}, \n",
    "               next_obs = {'raycast': next_agent_obs[0], 'state': next_agent_obs[1]},\n",
    "               action = np.array(prev_agent_obs[3]), \n",
    "               reward = next_agent_obs[2], \n",
    "               done = next_agent_obs[4],\n",
    "               infos = [{}])\n",
    "        \n",
    "    # crucial step, easy to overlook, update the previous observation\n",
    "    obs = next_obs\n",
    "    \n",
    "    # Training loop\n",
    "    for _ in range(args.update_per_step):\n",
    "\n",
    "        # Log episodic stats periodically\n",
    "        if episodic_stats is not None and global_step % args.metrics_log_interval == 0:\n",
    "            print_text = f\"[{global_step}/{args.total_timesteps}] \"\n",
    "            for s in episodic_stats:\n",
    "                writer.add_scalar(\"episodic_stats/\" + s, episodic_stats[s], global_step)\n",
    "                print_text += f\"|{s}: {episodic_stats[s]:.5f}\"\n",
    "            print_text += f'| SPS: {int(global_step / (time.time() - start_time))}'\n",
    "            print(print_text)\n",
    "\n",
    "        # Save best models based on reward\n",
    "        if episodic_stats is not None and episodic_stats[\"reward\"] > best_reward:\n",
    "            best_reward = episodic_stats[\"reward\"]\n",
    "            torch.save(actor.state_dict(), os.path.join(save_path, 'actor_best.pth'))\n",
    "            for i, qf in enumerate(qf_ensemble):\n",
    "                torch.save(qf.state_dict(), os.path.join(save_path, f'qf{i+1}_best.pth'))\n",
    "            for i, qft in enumerate(qf_ensemble_target):\n",
    "                torch.save(qft.state_dict(), os.path.join(save_path, f'qf{i+1}_target_best.pth'))\n",
    "\n",
    "        # Start learning after a warm-up phase\n",
    "        if global_step > args.learning_starts:\n",
    "\n",
    "            # Sample a batch from replay buffer\n",
    "            data = rb.sample(args.batch_size)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Compute target action with exploration noise\n",
    "                next_action, next_log_pi, _, _ = actor.get_action(\n",
    "                    data.next_observations['raycast'], \n",
    "                    data.next_observations['state']\n",
    "                )\n",
    "\n",
    "                if args.noise_clip > 0:\n",
    "                    noise = torch.randn_like(next_action) * args.noise_clip\n",
    "                    next_action = torch.clamp(next_action + noise, -1, 1)\n",
    "\n",
    "                # Compute target Q-value (min over ensemble)\n",
    "                target_q_values = []\n",
    "                for q_target in qf_ensemble_target:\n",
    "                    q_val = q_target(\n",
    "                        data.next_observations['raycast'], \n",
    "                        data.next_observations['state'], \n",
    "                        next_action\n",
    "                    )\n",
    "                    target_q_values.append(q_val)\n",
    "                stacked_target_q = torch.stack(target_q_values)\n",
    "                min_qf_next_target = stacked_target_q.min(dim=0).values - alpha * next_log_pi\n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * min_qf_next_target.view(-1)\n",
    "\n",
    "            # Q-function updates (with bootstrapping)\n",
    "            q_losses = []\n",
    "            q_vals = []\n",
    "            batch_size = int(data.actions.shape[0] * args.bootstrap_batch_proportion)\n",
    "            for q in qf_ensemble:\n",
    "                # Bootstrap indices\n",
    "                indices = torch.randint(0, batch_size, (batch_size,), device=data.actions.device)\n",
    "                \n",
    "                obs_raycast = data.observations['raycast'][indices]\n",
    "                obs_state = data.observations['state'][indices]\n",
    "                actions = data.actions[indices]\n",
    "                target = next_q_value[indices]\n",
    "\n",
    "                # Compute Q loss\n",
    "                q_val = q(obs_raycast, obs_state, actions).view(-1)\n",
    "                loss = F.mse_loss(q_val, target)\n",
    "                q_losses.append(loss)\n",
    "                q_vals.append(q_val)\n",
    "                \n",
    "            total_q_loss = torch.stack(q_losses).mean()\n",
    "            qf_optimizer.zero_grad()\n",
    "            total_q_loss.backward()\n",
    "            qf_optimizer.step()\n",
    "            \n",
    "            # Track Q-value statistics\n",
    "            q_std = torch.stack(q_vals).std(dim=0).mean().item()\n",
    "            q_vals = torch.stack(q_vals).mean()\n",
    "            \n",
    "            # Delayed policy (actor) update\n",
    "            if global_step % args.policy_frequency == 0:\n",
    "                for _ in range(args.policy_frequency):\n",
    "                    pi, log_pi, _, _ = actor.get_action(data.observations['raycast'], data.observations['state'])\n",
    "                    actor_entropy = - (log_pi.exp() * log_pi).sum(dim=-1).mean()\n",
    "\n",
    "                    q_pi_vals = [q(data.observations['raycast'], data.observations['state'], pi) for q in qf_ensemble]\n",
    "                    min_qf_pi = torch.min(torch.stack(q_pi_vals), dim=0).values.view(-1)\n",
    "\n",
    "                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "\n",
    "                    # Automatic entropy tuning (if enabled)\n",
    "                    if args.autotune:\n",
    "                        with torch.no_grad():\n",
    "                            _, log_pi, _, _ = actor.get_action(data.observations['raycast'], data.observations['state'])\n",
    "                        alpha_loss = (-log_alpha * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                        a_optimizer.zero_grad()\n",
    "                        alpha_loss.backward()\n",
    "                        a_optimizer.step()\n",
    "                        alpha = log_alpha.exp().item()\n",
    "\n",
    "            # Soft update target Q-networks\n",
    "            if global_step % args.target_network_frequency == 0:\n",
    "                for q, q_t in zip(qf_ensemble, qf_ensemble_target):\n",
    "                    for param, target_param in zip(q.parameters(), q_t.parameters()):\n",
    "                        target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            # Log training losses and stats\n",
    "            if global_step % args.loss_log_interval == 0:\n",
    "                for i in range(len(qf_ensemble)):\n",
    "                    writer.add_scalar(f\"q_loss/qf{i+1}\", q_losses[i].item(), global_step)\n",
    "\n",
    "                writer.add_scalar(\"loss/qf_mean\", torch.stack(q_losses).mean().item(), global_step)\n",
    "                writer.add_scalar(\"loss/actor\", actor_loss.item(), global_step)\n",
    "                \n",
    "                writer.add_scalar(\"stats/qf_val_mean \", q_vals, global_step)\n",
    "                writer.add_scalar(\"stats/qf_val_var \", q_std, global_step)\n",
    "                \n",
    "                writer.add_scalar(\"stats/policy_entropy\", actor_entropy.item(), global_step)\n",
    "                writer.add_scalar(\"stats/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "                if args.autotune:\n",
    "                    writer.add_scalar(\"loss/alpha\", alpha, global_step)\n",
    "                    writer.add_scalar(\"loss/alpha_loss\", alpha_loss.item(), global_step)\n",
    "\n",
    "        elif global_step == args.learning_starts:\n",
    "            print(\"Start Learning\")\n",
    "\n",
    "        # Step counter\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c8b7d",
   "metadata": {},
   "source": [
    "# Close Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a243c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained networks, actor and critics\n",
    "torch.save(actor.state_dict(), os.path.join(save_path, 'actor_final.pth'))\n",
    "for i, qf in enumerate(qf_ensemble):\n",
    "    torch.save(qf.state_dict(), os.path.join(save_path, f'qf{i+1}_final.pth'))\n",
    "for i, qft in enumerate(qf_ensemble_target):\n",
    "    torch.save(qft.state_dict(), os.path.join(save_path, f'qf{i+1}_target_final.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
