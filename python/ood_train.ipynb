{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf6d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from my_utils import *\n",
    "\n",
    "# === Local application-specific imports ===\n",
    "from utils_uf_methods import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4d3c2",
   "metadata": {},
   "source": [
    "# Loading Dataset for Training and Hold-Out\n",
    "\n",
    "Below I load into memory the data that will be used for training and later to compute statistics useful for OOD detection.  \n",
    "These data are observations collected from a reinforcement learning agent running on the standard environment after training is completed.  \n",
    "They consist of sequences divided into episodes, which are then shuffled together.\n",
    "\n",
    "The data will also be split into two groups based on the outcome of the recorded episode.  \n",
    "My intention is to train two different models: one trained on all the data, and a second trained only on states from episodes where the agent successfully reached the goal.\n",
    "\n",
    "This distinction is important because the agent does not always succeed, and when it fails, episodes tend to last many more steps, during which the agent either gets stuck in walls or cannot escape from a local minimum.  \n",
    "As a result, the dataset contains many \"unusual\" or outlier-like states, and it will be interesting to study how the absence of these states affects the performance of the model and the overall analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Print CUDA-related details for reproducibility/debugging\n",
    "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    # Inform user when running only on CPU\n",
    "    print(\"CUDA not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d3d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from JSON file\n",
    "with open('./u_e_test/datasets/standard_2200087.json') as f:\n",
    "    raw_dataset = json.load(f)\n",
    "\n",
    "# raw_dataset is a list of episodes\n",
    "# Each episode is itself a list of steps\n",
    "# Each step has the format:\n",
    "# [observation_list, success_flag, episode_length, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e5537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.tensor([step[:-2] for episodio in raw_dataset for step in episodio]) # remove action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e35d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion: tr=0.6 / val=0.2 / ts=0.2 \n",
    "states = split_dataset(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small constant to prevent division by zero in std\n",
    "STD_EPSILON = 1e-6\n",
    "\n",
    "# Compute mean and std from TRAIN set only (index 0)\n",
    "states_mean = states[0].mean(dim=0, keepdim=True)\n",
    "states_std = states[0].std(dim=0, keepdim=True) + STD_EPSILON\n",
    "\n",
    "# Keep a copy of calibration set without standardization\n",
    "calibration_not_std = states[2]\n",
    "\n",
    "# Standardize all splits (train, val, cal, test)\n",
    "for i in range(len(states)):\n",
    "    states[i] = (states[i] - states_mean) / states_std\n",
    "\n",
    "# Wrap splits into dataset objects\n",
    "states_dataset = (\n",
    "    FlatDataset(states[0]),  # train\n",
    "    FlatDataset(states[1]),  # val\n",
    "    FlatDataset(states[2]),  # cal\n",
    "    FlatDataset(states[3])   # test\n",
    ")\n",
    "calibration_not_std_dataset = FlatDataset(calibration_not_std)\n",
    "\n",
    "# Create dataloaders\n",
    "states_loader = (\n",
    "    DataLoader(states_dataset[0], batch_size=256, shuffle=True),\n",
    "    DataLoader(states_dataset[1], batch_size=256, shuffle=True),\n",
    "    DataLoader(states_dataset[2], batch_size=256, shuffle=True),\n",
    "    DataLoader(states_dataset[3], batch_size=256, shuffle=True)\n",
    ")\n",
    "calibration_not_std_loader = DataLoader(calibration_not_std_dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5889f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b441b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids\n",
    "lrs = [0.0005, 0.0001, 0.001]   # learning rates\n",
    "wds = [1e-6, 1e-5, 1e-7]        # weight decays\n",
    "\n",
    "'''\n",
    "# Grid search over learning rates and weight decays\n",
    "for l in lrs:\n",
    "    for w in wds:\n",
    "        train_rnd(\n",
    "            \"rnd\",              # prefix name\n",
    "            states_loader[0],   # train set\n",
    "            states_loader[1],   # validation set\n",
    "            device,             # CPU/GPU\n",
    "            [128,128,128],      # hidden layers\n",
    "            500,                # max epochs\n",
    "            l,                  # learning rate\n",
    "            w,                  # weight decay\n",
    "            8                   # patience\n",
    "        )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf43ee",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved RND model parameters\n",
    "rnd_params = torch.load('./u_e_test/ood_models/rnd_7346509_uptight_vortex.pth')\n",
    "\n",
    "# Initialize networks: predictor (trainable) and source (fixed random target)\n",
    "rnd_source = RNDNetwork(96, [128,128,128])\n",
    "rnd_predictor = RNDNetwork(96, [128,128,128])\n",
    "\n",
    "# Load trained weights into the predictor and fixed weights into the source\n",
    "rnd_predictor.load_state_dict(rnd_params['predictor_state_dict'])\n",
    "rnd_source.load_state_dict(rnd_params['random_state_dict'])\n",
    "\n",
    "# Set both networks to evaluation mode (disable dropout, BN updates, etc.)\n",
    "rnd_predictor.eval()\n",
    "rnd_source.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea986fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_uncertainties_rnd(source, predictor, data_loader):\n",
    "    \"\"\"\n",
    "    Compute RND uncertainties as the squared difference between\n",
    "    the fixed random network (source) and the trained predictor.\n",
    "    \"\"\"\n",
    "    all_diffs = []\n",
    "    source.eval()\n",
    "    predictor.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            # Forward pass through both networks\n",
    "            pred_source = source(x)\n",
    "            pred_predictor = predictor(x)\n",
    "            \n",
    "            # Element-wise squared difference\n",
    "            diff = (pred_source - pred_predictor) ** 2\n",
    "            \n",
    "            # Aggregate across features â†’ one uncertainty score per sample\n",
    "            diff = diff.sum(dim=1) * 100  # scaling factor\n",
    "            \n",
    "            all_diffs.append(diff.detach().cpu())\n",
    "    \n",
    "    # Concatenate all batches into a single array\n",
    "    all_diffs = torch.cat(all_diffs, dim=0)\n",
    "    return all_diffs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece50b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_percentiles(name, data):\n",
    "    \"\"\"\n",
    "    Compute and print selected percentiles of the input data.\n",
    "    \"\"\"\n",
    "    # Percentiles to compute\n",
    "    percentiles = torch.tensor(\n",
    "        [1,10,20,30,40,50,60,65,70,75,80,85,90,95,99],\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # Ensure input is a torch tensor\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        data = torch.from_numpy(data)\n",
    "\n",
    "    # Compute percentile values\n",
    "    values = torch.quantile(data, percentiles / 100.0)\n",
    "\n",
    "    # Pair percentile values with their results\n",
    "    results = list(zip(percentiles.tolist(), values.tolist()))\n",
    "    \n",
    "    # Print formatted output\n",
    "    print(f\"{name} percentiles:\")\n",
    "    for x in results:\n",
    "        print(f\"  {x[0]}th percentile: {x[1]}\")\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prct = print_percentiles('rnd',  compute_uncertainties_rnd(rnd_source, rnd_predictor, states_loader[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RND model configuration\n",
    "args = {\n",
    "    'input_dim': 96,\n",
    "    'hidden_layers': [128, 128, 128],\n",
    "}\n",
    "\n",
    "# Save RND checkpoint with source/predictor weights, normalization stats, and percentiles\n",
    "torch.save({\n",
    "    'model_parameters': [rnd_source.state_dict(),  # fixed random network\n",
    "                         rnd_predictor.state_dict()],  # trained predictor\n",
    "    'model_args': args,                              # architecture settings\n",
    "    'input_mean': states_mean,                       # normalization mean (train set)\n",
    "    'input_std': states_std,                         # normalization std (train set)\n",
    "    'percentiles': prct                              # uncertainty distribution\n",
    "}, './u_e_test/rnd_method.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
